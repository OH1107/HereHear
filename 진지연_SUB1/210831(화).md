### 08.31(화)

#### 워드 임베딩

- 추천 알고리즘을 구현하되, 인공지능의 끈을 놓지 말자.
- 추천 알고리즘에서 상품의 설명을 벡터로 만들 때 자연어처리를 응용할 수 있다.
- count vectorizer : 문단에서 등장한 단어 ➡️  원핫인코딩 ➡️ 벡터
- tdidf vectorizer : 문단에서 등장한 단어 ➡️ 모든 문단에서 공통으로 등장하는 단어는 제외 ➡️ 원핫인코딩 ➡️ 벡터
- CBoW, skip-gram : 주변이 유사한 토큰끼리는 유사한 벡터들로 표현한다. 원핫인코딩이 아닌 (0.2 0.8 0.3 0.4 0.5) 의 형태.



#### 자동 태그 생성 기능

자동태그생성 기능은, 문단에서 중심단어를 찾아 요약해주는 것이다.

- 직접 설계하는 경우

  -  `데이터:라벨값 = 문장:중심단어`로 구성된 데이터

    중심단어로 라벨링되어 있는 데이터를 찾아야 함

  - 이것을 RNN, LSTM, Attention, Trasformer 등으로 학습한다

- API(외부에서 만든 것)를 활용하는 경우

  - 구글에서 만든 BERT를 변형하여 활용한다

  방법1) 

  - 태그가 정해져 있는 경우, 문장분류기를 응용한 것으로 이해할 수 있다.
  - 직접 설계하는 경우와 마찬가지로 중심단어가 라벨링되어 있는 데이터를 찾아야 한다

  - BERT로 전체 문단의 벡터를 얻는다

  - ➡️ 분류 모델을 더하여 자동으로 분류한다

  

  방법2) 

  - 태그가 정해져 있지 않는 경우, 추천 알고리즘과 비슷하게 중심단어를 찾는다
  - 방법1과 달리 중심단어가 라벨링되어 있지 않아도 데이터를 찾을 수 있다.

  - BERT로 전체 문단의 벡터를 얻는다
  - ➡️ BERT로 문단을 구성하는 각 단어의 벡터를 얻는다
  - ➡️ 가장 비슷한 단어를 추출한다



#### BERT

- BERT는 손쉽게 불러올 수 있다. 
- 아래는 방법1을 사용하는 예시

```
pip install transformers

from transformers import BertModel
bert = BertModel.from_pretrained('bert-base-uncased')
model_config['after_vectorizer'] = bert.config.to_dict()['hidden_size']

class SentenceClassification(nn.Module)
	def __init__(self, **model__config):
		super(SentenceClassification, self).__init__()
		self.bert = bert
		self.fc = nn.Linear(model_config['after_vectorizer'], model_config['output'])
	def forward(self, x):
		pooled_cls_output = self.bert(x)[1]
		return self.fc(pooled_cls_output)
```

- BERT는 문장의 시작과 끝을 표현하기 위해 [CLS], [ESP] 를 명시했다. 이에 대한 전처리가 필요하다.



한국어용 BERT에 대해서도 좀더 알아볼 필요가 있다.

