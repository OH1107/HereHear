### 09.03.금

#### CNN

- CNN에서 숫자들을 계산하는 방법에 대해 조금더 조사해보았다.
- forward > view에서 8 * 8 * 16인 이유에 대해서 알아보자



- 맨 처음에 이미지 사이즈는 [가로:32, 세로:32, 채널:3]이다

  ```
  images.size()
  > torch.Size([32, 3, 32, 32])
  ```

  

- conv1을 지나면 [가로:32, 세로:32, 채널:8]이 된다

- kernel로 훑으면서 가로와 세로가 줄어들지만, padding을 1로 주어 줄어들지 않았다.

- padding 값은 가로 세로가 줄어들지 않도록 하는 값을 넣어주어야 한다.

  ```
  self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1)
  self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)
  ```

  

- pool을 지나면 [가로:16, 세로:16, 채널:8]이 된다
- pool의 kernel 사이즈가 2, stride가 2이므로 그렇다.



- 실전에서는.. 헷갈리기 때문에 중간중간 .size나 .shape를 찍으며 확인해야 한다.

- input값, padding은 정확히 계산해주어야 한다
- out_channel은 보통 이미지 사이즈와 반비례하게끔 설정해준다.



#### 워드 임베딩

pre_trained된 모델을 활용해 형태소를 추출하고 단어를 벡터로 바꾸자. 

워드 임베딩 모델을 학습시키려면 수많은 문장을 수집하고, 신경망도 촘촘히 쌓아야 한다. 직접 한다고 해서 성능이 좋게 나올지도 의문. 다행히 pytorch에서 pre_trained 모델을 제공한다.

- BERT tokenizer

  토크나이징 한 결과가 부자연스럽다. 토크나이징 단계에서 부자연스럽더라도 실제 모델에서 잘 나올 수도 있는지 조사가 필요하다

```python
kor_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')
kor_sentence = '나는 책상 위에 사과를 먹었다. 알고 보닌 그 사과는 지연의 것이었다. 그래서 지연이에게 사과했다'
print(tokenizer.tokenize(kor_sentence))
> ['ᄂ', '##ᅡ', '##ᄂ', '##ᅳ', '##ᆫ', 'ᄎ', '##ᅢ', '##ᆨ', '##ᄉ', '##ᅡ', '##ᆼ', '[UNK]', 'ᄉ', '##ᅡ', '##ᄀ', '##ᅪ', '##ᄅ', '##ᅳ', '##ᆯ', '[UNK]', '.', 'ᄋ', '##ᅡ', '##ᆯ', '##ᄀ', '##ᅩ', 'ᄇ', '##ᅩ', '##ᄂ', '##ᅵ', '##ᆫ', 'ᄀ', '##ᅳ', 'ᄉ', '##ᅡ', '##ᄀ', '##ᅪ', '##ᄂ', '##ᅳ', '##ᆫ', 'ᄌ', '##ᅵ', '##ᄋ', '##ᅧ', '##ᆫ', '##ᄋ', '##ᅴ', '[UNK]', '.', 'ᄀ', '##ᅳ', '##ᄅ', '##ᅢ', '##ᄉ', '##ᅥ', 'ᄌ', '##ᅵ', '##ᄋ', '##ᅧ', '##ᆫ', '##ᄋ', '##ᅵ', '##ᄋ', '##ᅦ', '##ᄀ', '##ᅦ', '[UNK]']
```



- 워드 임베딩

  토크나이징한 단어를 벡터로 변환한다.

  glove.6B.300d는 스탠포드에서 학습시킨 모델로 의미를 담은 벡터로 변환한다. [공홈](https://nlp.stanford.edu/projects/glove/) king-man+woman = queen이 나오는 벡터라는 뜻. 아무튼 영어는 잘 되는데, 한국어도 잘 되는지 찾아봐야겠다.

```python
TEXT.build_vocab(train_data,
                 min_freq = 2,     # 두번 이상 등장한 단어만 임베딩하겠다. 한번 등장한 단어는 임베딩하지 않겠다.
                 max_size = None,
                 vectors = "glove.6B.300d"
                 )

LABEL.build_vocab(train_data)
```

