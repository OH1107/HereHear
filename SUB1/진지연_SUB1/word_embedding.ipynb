{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embedding.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqUT-e6IJuGm"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtLKosiMtlf"
      },
      "source": [
        "BERT- multilingual tokenizer 는 한국어 단어를 부자연스럽게 쪼갠다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sffybh9fJzHm"
      },
      "source": [
        "from transformers import BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71spOsIzJ-cz"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "eng_sentence = 'My dog is cute. He likes playing'\n",
        "print(tokenizer.tokenize(eng_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXUp0qmjKpgy"
      },
      "source": [
        "kor_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "kor_sentence = '나는 책상 위에 사과를 먹었다. 알고 보닌 그 사과는 지연의 것이었다. 그래서 지연이에게 사과했다'\n",
        "print(tokenizer.tokenize(kor_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI5eVkV-NNCy"
      },
      "source": [
        "활용할 데이터는 영화 커뮤니티의 리뷰 댓글로, 긍정 / 부정에 대한 라벨링이 되어 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4ZpFhC6NUBh"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMH-8VCmNZAy"
      },
      "source": [
        "TEXT = data.Field(batch_first = True,\n",
        "                  fix_length = 500,\n",
        "                  tokenize = str.split,\n",
        "                  pad_first = True,\n",
        "                  pad_token = '[PAD]',\n",
        "                  unk_token = '[UNK]')\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(text_field = TEXT,\n",
        "                                             label_field = LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGmg1LgiOBq4"
      },
      "source": [
        "print('data')\n",
        "print(' '.join(vars(train_data.examples[1])['text']))\n",
        "print('label')\n",
        "print(vars(train_data.examples[1])['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2KwQtR-VnHl"
      },
      "source": [
        "정규표현식을 활용해서 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGiLDb-8VwBO"
      },
      "source": [
        "import re\n",
        "\n",
        "def PreProcessingText(input_sentence):\n",
        "    input_sentence = input_sentence.lower() # 소문자화\n",
        "    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) # \"<br />\" 처리\n",
        "    input_sentence = re.sub('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]', repl= ' ', string = input_sentence) # 특수문자 처리 (\"'\" 제외)\n",
        "    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리\n",
        "    if input_sentence:\n",
        "        return input_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-xYmdBNWzzf"
      },
      "source": [
        "for example in train_data.examples:\n",
        "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n",
        "    \n",
        "for example in test_data.examples:\n",
        "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqJZInYBeHTK"
      },
      "source": [
        "형태소를 추출하여 임베딩하는 모델은 스탠포드에서 학습시킨 'globe.6B.300d'를 활용했다.\n",
        "내가 하면 등장빈도로 원핫인코딩 정도만 하겠지만.. \n",
        "pre_trained vector는 의미를 담은 벡터로 변환한다. https://nlp.stanford.edu/projects/glove/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn77iDuaXBqW"
      },
      "source": [
        "TEXT.build_vocab(train_data,\n",
        "                 min_freq = 2,                     # 두번 이상 등장한 단어만 임베딩하겠다. 한번 등장한 단어는 임베딩하지 않겠다.\n",
        "                 max_size = None,\n",
        "                 vectors = \"glove.6B.300d\"\n",
        "                 )\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLM1lLA9euC2"
      },
      "source": [
        "print(f'Vocab Size : {len(TEXT.vocab)}')\n",
        "\n",
        "print('Vocab Examples : ')\n",
        "for idx, (k, v) in enumerate(TEXT.vocab.stoi.items()):\n",
        "    if idx >= 10:\n",
        "        break    \n",
        "    print('\\t', k, v)\n",
        "\n",
        "print('---------------------------------')\n",
        "\n",
        "# Label Info\n",
        "print(f'Label Size : {len(LABEL.vocab)}')\n",
        "\n",
        "print('Lable Examples : ')\n",
        "for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n",
        "    print('\\t', k, v)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}